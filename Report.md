# MAP55616-03 - CUDA Exponential Integral Calculation Report

## ğŸ‘¤ Student Information

* **Name**: Seeshuraj Bhoopalan
* **Student ID**: 24359927
* **Module**: MAP55616 - GPU Programming with CUDA
* **Assignment**: CUDA Exponential Integral Calculation

---

## ğŸ§  Task 1: CUDA Implementation Summary

### ğŸ¯ Objective

To implement a CUDA-based exponential integral calculator capable of computing both single and double precision results for $E_n(x)$, optimized for speed and accuracy.

### ğŸ› ï¸ Implementation Details

* **Base Code Used**: `exponentialIntegralCPU.tar`
* **Files Added**:

  * `gpu_integral.cu`, `gpu_integral.h`: CUDA kernel logic
  * `cpu_integral.cpp`, `cpu_integral.h`: Reorganized CPU logic
  * `main.cpp`: Command-line parser for flags `-c`, `-g`, `-n`, `-m`
  * `Makefile`: NVCC-compatible build script

### ğŸš€ Feature Summary

* Separate CUDA kernels for float and double
* Timing includes *all* CUDA steps (allocations, transfers, compute)
* Output compared using `fabs(cpu - gpu) < 1e-5`
* Supports CPU-only, GPU-only, or both (auto benchmark mode)

### â–¶ï¸ Execution Examples

```bash
./ei_exec -c -n 8192 -m 8192      # CPU only
./ei_exec -g -n 8192 -m 8192      # GPU only
./ei_exec -n 8192 -m 8192         # CPU + GPU + Speedup comparison
```

---

## ğŸ“Š Performance Benchmarks

| Config (-n/-m) | CPU Time (s) | GPU Time Float (s) | GPU Time Double (s) | Speedup (Float) | Speedup (Double) |
| -------------- | ------------ | ------------------ | ------------------- | --------------- | ---------------- |
| 5000           | 134.21       | 0.303              | 11.21               | 443.07          | 11.97            |
| 8192           | 353.04       | 0.721              | 29.16               | 489.18          | 12.10            |
| 16384          | 1412.29      | 1.439              | 116.20              | 981.40          | 12.15            |
| 20000          | 2112.33      | 1.698              | 161.50              | 1243.69         | 13.08            |

---

## âœ… Numerical Accuracy Check

The CPU and GPU results were compared for every element in the `n Ã— m` matrix using:

```cpp
if (fabs(cpu_result[i] - gpu_result[i]) > 1e-5) {
    std::cerr << "Mismatch at " << i << ": CPU=" << cpu_result[i]
              << ", GPU=" << gpu_result[i] << std::endl;
}
```

**âœ… No mismatches** were observed across all test cases.

---

## ğŸ§ª Non-Square Test Case

Tested:

```bash
./ei_exec -n 10000 -m 5000
```

**âœ… Output and speedup correct. Accuracy within threshold.**

---

## ğŸ”§ CUDA Features Used

* `cudaMalloc`, `cudaMemcpy`, `cudaFree`
* `cudaEvent_t` for timing precision
* Grid/block configuration tuning
* No external libraries used

---

## ğŸ¤– Task 2: LLM Implementation & Comparison

### ğŸ“‹ LLM Used

* **LLM**: ChatGPT-4 (OpenAI)

### ğŸ“Œ Prompt

> "Convert this CPU loop and exponential integral function to an optimized CUDA kernel"

### ğŸ§  LLM Output

* Recommended `__global__` kernel with 2D indexing
* Suggested shared memory usage (manually confirmed no speedup)
* Properly handled memory allocation and launch configuration
* Accurate transformation of loop into `i * m + j` indexing

### âœ… Results

* **Correctness**: Output matched CPU results
* **Performance**: Similar to manually written kernel
* **Conclusion**: Helpful but no performance gain over manual version

---

## ğŸ”– Git Progress Tags (Proof of Work)

You can find clear commit history in `commit_log.txt` (auto-generated via `git log --oneline`), including:

* `cpu-working`: Initial CPU version
* `cuda-transfer-device`: Memory allocations and transfers
* `basic-gpu-impl`: Kernel implementation (float and double)
* `timing-verified`: Benchmarking framework complete
* `report-complete`: Report + plot ready

---

## ğŸ“‰ Observations

* Float kernel shows **400â€“1200Ã— speedup**
* Double kernel is **\~12Ã— faster** than CPU version
* CUDA `cold-start` penalty was noticed in the first run
* Code handles **non-square matrices** correctly

---

## ğŸ“‚ Files Submitted

* Source Code: `main.cpp`, `cpu_integral.*`, `gpu_integral.*`, `Makefile`
* Benchmark Script: `plot_speedup.py`, `speedup_plot.png`
* Documentation: `Report.md`, `README.md`, `commit_log.txt`

---

## âœ… Final Notes

This assignment deepened my understanding of:

* GPU memory hierarchy and management
* Float vs Double compute behavior
* Speedup measurement using CUDA Events
* Responsible benchmarking
* Interpreting and critiquing LLM outputs

---

**End of Report**
